{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJLw3mUZ8dB8",
        "outputId": "20e3e454-d605-49cd-b0a9-58cfb08b2eb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted files: ['SimLex-999']\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "\n",
        "zip_path = '/content/SimLex-999.zip'\n",
        "\n",
        "\n",
        "extract_path = '/content/data/'\n",
        "\n",
        "\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Extract the ZIP file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "\n",
        "extracted_files = os.listdir(extract_path)\n",
        "print(\"Extracted files:\", extracted_files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxMVFnXh9_aT",
        "outputId": "70e9f79c-4173-4e99-877d-2fe8d3d91af8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the first word: happy\n",
            "Enter the second word: funny\n",
            "Similarity score between 'happy' and 'funny': 0.09615021198987961\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/data/SimLex-999/SimLex-999.txt', delimiter='\\t')\n",
        "\n",
        "# Convert POS column to lowercase\n",
        "data['POS'] = data['POS'].str.lower()\n",
        "\n",
        "# Train Word2Vec model\n",
        "corpus = [word_tokenize(sentence.lower()) for sentence in data['word1'] + ' ' + data['word2']]\n",
        "model = Word2Vec(corpus, min_count=1)\n",
        "\n",
        "# Function to calculate similarity score\n",
        "def get_similarity_score(word1, word2):\n",
        "    if word1 in model.wv.key_to_index and word2 in model.wv.key_to_index:\n",
        "        vec1 = model.wv[word1].reshape(1, -1)\n",
        "        vec2 = model.wv[word2].reshape(1, -1)\n",
        "        return cosine_similarity(vec1, vec2)[0][0]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Get user input for two words\n",
        "word1 = input(\"Enter the first word: \").lower()\n",
        "word2 = input(\"Enter the second word: \").lower()\n",
        "\n",
        "# Calculate similarity score for the input words\n",
        "similarity_score = get_similarity_score(word1, word2)\n",
        "\n",
        "if similarity_score is not None:\n",
        "    print(f\"Similarity score between '{word1}' and '{word2}': {similarity_score}\")\n",
        "else:\n",
        "    print(\"One or both of the words are not in the vocabulary.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSczPo2ce9Xq",
        "outputId": "e5b82a7b-c5c3-4ad8-8cd0-1be0d510f4f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pearson correlation coefficient: -0.015247346228155398\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# Load the SimLex-999 dataset\n",
        "data = pd.read_csv('/content/data/SimLex-999/SimLex-999.txt', delimiter='\\t')\n",
        "\n",
        "# Load a small monolingual English corpus\n",
        "with open('/content/Pri.txt', 'r') as f:\n",
        "    corpus_text = f.read()\n",
        "\n",
        "# Tokenize the corpus\n",
        "corpus_tokens = [word_tokenize(sentence.lower()) for sentence in corpus_text.split('.')]\n",
        "\n",
        "# Train Word2Vec model on the corpus\n",
        "model = Word2Vec(corpus_tokens, min_count=1)\n",
        "\n",
        "# Function to calculate similarity score\n",
        "def get_similarity_score(word1, word2):\n",
        "    if word1 in model.wv.key_to_index and word2 in model.wv.key_to_index:\n",
        "        vec1 = model.wv[word1].reshape(1, -1)\n",
        "        vec2 = model.wv[word2].reshape(1, -1)\n",
        "        return cosine_similarity(vec1, vec2)[0][0]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Calculate similarity scores for all pairs in the dataset\n",
        "data['predicted_score'] = data.apply(lambda x: get_similarity_score(x['word1'], x['word2']), axis=1)\n",
        "\n",
        "# Evaluate the model using a suitable metric\n",
        "pearson_corr = data['predicted_score'].corr(data['SimLex999'], method='pearson')\n",
        "\n",
        "print(f\"Pearson correlation coefficient: {pearson_corr}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CQmb6z0mOit",
        "outputId": "bc0b15ec-6dbd-425f-cfdf-2344df2e736a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SM3g59uRf2_k",
        "outputId": "3ad2a376-4fb7-4322-db75-f193ffccb55e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pearson correlation coefficient: -0.021811329695624847\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# Load the SimLex-999 dataset\n",
        "data = pd.read_csv('/content/data/SimLex-999/SimLex-999.txt', delimiter='\\t')\n",
        "\n",
        "# Load a small monolingual English corpus\n",
        "# with open('/content/Uly.txt', 'r') as f:\n",
        "#     corpus_text = f.read()\n",
        "\n",
        "with open('/content/Pri.txt', 'r') as f:\n",
        "    corpus_text = f.read()\n",
        "\n",
        "# Tokenize the corpus\n",
        "corpus_tokens = [word_tokenize(sentence.lower()) for sentence in corpus_text.split('.')]\n",
        "\n",
        "# Train Word2Vec model on the corpus\n",
        "model = Word2Vec(corpus_tokens, min_count=1)\n",
        "\n",
        "# Function to calculate similarity score\n",
        "def get_similarity_score(word1, word2):\n",
        "    if word1 in model.wv.key_to_index and word2 in model.wv.key_to_index:\n",
        "        vec1 = model.wv[word1].reshape(1, -1)\n",
        "        vec2 = model.wv[word2].reshape(1, -1)\n",
        "        return cosine_similarity(vec1, vec2)[0][0]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Calculate similarity scores for all pairs in the dataset\n",
        "data['predicted_score'] = data.apply(lambda x: get_similarity_score(x['word1'], x['word2']), axis=1)\n",
        "\n",
        "# Evaluate the model using a suitable metric\n",
        "pearson_corr = data['predicted_score'].corr(data['SimLex999'], method='pearson')\n",
        "\n",
        "print(f\"Pearson correlation coefficient: {pearson_corr}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3yyTl7fp04m",
        "outputId": "e3c4e0f6-123a-4aed-84e2-d3ae438bf5f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pearson correlation coefficient (Unconstrained Setting): 0.021930793576314757\n"
          ]
        }
      ],
      "source": [
        "# Train Word2Vec model on a larger corpus (e.g., Wikipedia or Google News)\n",
        "model_unconstrained = Word2Vec(corpus_tokens, min_count=1)\n",
        "\n",
        "# Function to calculate similarity score\n",
        "def get_similarity_score_unconstrained(word1, word2):\n",
        "    if word1 in model_unconstrained.wv.key_to_index and word2 in model_unconstrained.wv.key_to_index:\n",
        "        vec1 = model_unconstrained.wv[word1].reshape(1, -1)\n",
        "        vec2 = model_unconstrained.wv[word2].reshape(1, -1)\n",
        "        return cosine_similarity(vec1, vec2)[0][0]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Calculate similarity scores for all pairs in the dataset\n",
        "data['predicted_score_unconstrained'] = data.apply(lambda x: get_similarity_score_unconstrained(x['word1'], x['word2']), axis=1)\n",
        "\n",
        "# Evaluate the model using a suitable metric\n",
        "pearson_corr_unconstrained = data['predicted_score_unconstrained'].corr(data['SimLex999'], method='pearson')\n",
        "\n",
        "print(f\"Pearson correlation coefficient (Unconstrained Setting): {pearson_corr_unconstrained}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "5mpwoqfWqYv1"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"paws\", \"labeled_final\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpz1IvalrCle",
        "outputId": "c98610f8-d8fd-47d4-c356-cdcb25f86fe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.17.0-py3-none-any.whl (536 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/536.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m491.5/536.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Collecting pyarrow>=12.0.0 (from datasets)\n",
            "  Downloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 10.0.1\n",
            "    Uninstalling pyarrow-10.0.1:\n",
            "      Successfully uninstalled pyarrow-10.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 15.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.17.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-15.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ4_NeeCrTMI",
        "outputId": "a510cbf1-dcc5-4d23-d11d-6a1c84bf9b2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (15.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9R0yVEitndJP",
        "outputId": "ddd584c1-275a-4695-fd67-8299acb5caf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 15.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "CfXWFr8mpMj9"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset2 = load_dataset(\"PiC/phrase_similarity\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ge0u9u-u6RM",
        "outputId": "6c72c14d-bf6d-491a-d629-f05391bb763b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.24.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipympl\n",
            "  Downloading ipympl-0.9.3-py2.py3-none-any.whl (511 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/511.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m337.9/511.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.6/511.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipython<9 in /usr/local/lib/python3.10/dist-packages (from ipympl) (7.34.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ipympl) (1.24.4)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipympl) (0.2.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from ipympl) (9.4.0)\n",
            "Requirement already satisfied: traitlets<6 in /usr/local/lib/python3.10/dist-packages (from ipympl) (5.7.1)\n",
            "Requirement already satisfied: ipywidgets<9,>=7.6.0 in /usr/local/lib/python3.10/dist-packages (from ipympl) (7.7.1)\n",
            "Requirement already satisfied: matplotlib<4,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from ipympl) (3.7.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython<9->ipympl) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython<9->ipympl)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython<9->ipympl) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython<9->ipympl) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython<9->ipympl) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython<9->ipympl) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython<9->ipympl) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython<9->ipympl) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython<9->ipympl) (4.9.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=7.6.0->ipympl) (5.5.6)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=7.6.0->ipympl) (3.6.6)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=7.6.0->ipympl) (3.0.10)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.4.0->ipympl) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.4.0->ipympl) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.4.0->ipympl) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.4.0->ipympl) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.4.0->ipympl) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.4.0->ipympl) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.4.0->ipympl) (2.8.2)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (6.3.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython<9->ipympl) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython<9->ipympl) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<9->ipympl) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib<4,>=3.4.0->ipympl) (1.16.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (6.5.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (3.1.3)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (5.7.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (5.9.2)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.18.0)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.19.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.0.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (4.2.0)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.2.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (2.1.5)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.9.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (2.19.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (4.19.2)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (21.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.17.1)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.7.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.5.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (2.21)\n",
            "Installing collected packages: jedi, ipympl\n",
            "Successfully installed ipympl-0.9.3 jedi-0.19.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install nltk\n",
        "!pip3 install numpy\n",
        "import nltk\n",
        "import math\n",
        "import numpy as np\n",
        "import re\n",
        "import pprint\n",
        "import csv\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import brown\n",
        "from nltk.tokenize import word_tokenize\n",
        "!pip3 install ipympl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7gdzmDAWuooW"
      },
      "outputs": [],
      "source": [
        "# Input type : It takes 2 arguments with each Sentence string\n",
        "# Output type : list of words of the joint set\n",
        "\n",
        "def joint_set(sentence_1, sentence_2):\n",
        "    # words_1 = nltk.word_tokenize(sentence_1)\n",
        "    # words_2 = nltk.word_tokenize(sentence_2)\n",
        "    words_1 = sentence_1.split(\" \")\n",
        "    words_2 = sentence_2.split(\" \")\n",
        "    joint_words = []\n",
        "    for i in words_1:\n",
        "      if i not in joint_words:\n",
        "        joint_words.append(i)\n",
        "    for i in words_2:\n",
        "      if i not in joint_words:\n",
        "        joint_words.append(i)\n",
        "    return joint_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_E_g3Pvru0kY"
      },
      "outputs": [],
      "source": [
        "# Input type : 1 argument as a word\n",
        "# Output type : A score of Information Content from Brown Corpus\n",
        "\n",
        "N = 0\n",
        "brown_freqs = dict()\n",
        "ThresHold = 0.2\n",
        "\n",
        "def info_content(givenWord):\n",
        "    global N\n",
        "    if N == 0:\n",
        "        # print(\"Damm\")\n",
        "        for sentences in brown.sents():\n",
        "            for word in sentences:\n",
        "                word = word.lower()\n",
        "                if word not in brown_freqs:\n",
        "                    brown_freqs[word] = 0\n",
        "                brown_freqs[word] = brown_freqs[word] + 1\n",
        "                N = N + 1\n",
        "    givenWord = givenWord.lower()\n",
        "    n = 0 if givenWord not in brown_freqs else brown_freqs[givenWord]\n",
        "    return 1.0 - (math.log(n + 1) / math.log(N + 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-zR95tkfvFId"
      },
      "outputs": [],
      "source": [
        "# Input type : 2 argument with each as words\n",
        "# Output type : pair of synsets for the 2 input words\n",
        "\n",
        "def max_similarity_algo(word1, word2):\n",
        "    maxPair = [None, None]\n",
        "    maxSim = -1.0\n",
        "    for i in wn.synsets(word1):\n",
        "        for j in wn.synsets(word2):\n",
        "          sim = i.path_similarity(j)\n",
        "          if sim != None:\n",
        "            if sim > maxSim:\n",
        "              maxSim = sim\n",
        "              maxPair = i, j\n",
        "    return maxPair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "h0d3eeilu_Gr"
      },
      "outputs": [],
      "source": [
        "# Input type : 2 argument with each as words\n",
        "# Output type : Words Similarity score from wordnet\n",
        "\n",
        "def word_similarity(word1, word2):\n",
        "    synsetPair = max_similarity_algo(word1, word2)\n",
        "    if synsetPair == [None,None]:\n",
        "      return 0.0\n",
        "    return (synsetPair[0].path_similarity(synsetPair[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "W8jNPTNRuu8-"
      },
      "outputs": [],
      "source": [
        "# Input type : 2 argument with first as the list of words of the sentence whose semantic vector will be generated\n",
        "#                              second as the list of words of the joint set\n",
        "# Output type : a list of numbers representing the semantic Vector for the input sentence\n",
        "\n",
        "def semantic_vector(words, jointSetWords):\n",
        "    semantic_vec = [0] * len(jointSetWords)\n",
        "    idx = 0\n",
        "    for jointSetWord in jointSetWords:\n",
        "        if jointSetWord in words:\n",
        "            semantic_vec[idx] = 1.0\n",
        "            info_val = info_content(jointSetWord)\n",
        "            semantic_vec[idx] = semantic_vec[idx] * info_val * info_val\n",
        "        else:\n",
        "            maxSim = -1.0\n",
        "            simWord = \"\"\n",
        "            for word in words:\n",
        "              sim = word_similarity(word, jointSetWord)\n",
        "              if sim > maxSim:\n",
        "                maxSim = sim\n",
        "                simWord = word\n",
        "            if maxSim > ThresHold:\n",
        "              semantic_vec[idx] = maxSim\n",
        "            else:\n",
        "              semantic_vec[idx] = 0.0\n",
        "            semantic_vec[idx] = semantic_vec[idx] * info_content(jointSetWord) * info_content(simWord)\n",
        "        idx = idx + 1\n",
        "    return semantic_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GbsUolBMvLoB"
      },
      "outputs": [],
      "source": [
        "# Input type : 2 argument with first as the list of words of the sentence whose semantic vector will be generated\n",
        "#                              second as the list of words of the joint set\n",
        "# Output type : a list of numbers representing the Word Order Vector for the input sentence\n",
        "\n",
        "OrderThresHold = 0.4\n",
        "\n",
        "def wordOrder_vector(words, jointSetWords):\n",
        "    wordOrder_vec = [0] * len(jointSetWords)\n",
        "    idx = 0\n",
        "    for jointSetWord in jointSetWords:\n",
        "        if jointSetWord in words:\n",
        "            position = 0\n",
        "            for i in words:\n",
        "              position = position + 1\n",
        "              if i == jointSetWord:\n",
        "                break\n",
        "            wordOrder_vec[idx] = position\n",
        "        else:\n",
        "            maxSim = -1\n",
        "            position = 0\n",
        "            wordOrder_vec[idx] = 0\n",
        "            for i in words:\n",
        "              temp_similarity = word_similarity(i,jointSetWord)\n",
        "              if(temp_similarity >= OrderThresHold):\n",
        "                if(maxSim > temp_similarity):\n",
        "                  maxSim = temp_similarity\n",
        "                  wordOrder_vec[idx] = position\n",
        "              position = position + 1\n",
        "        idx = idx + 1\n",
        "    return wordOrder_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "vS5S97p4ufEB"
      },
      "outputs": [],
      "source": [
        "# Declaring a empty list\n",
        "# in case for similarity a single sentence is broken into multiple sentences, hences i will be removing fullStops\n",
        "# The same for the question mark are also what will be removed here\n",
        "\n",
        "# Input type : String\n",
        "# Output type : String\n",
        "\n",
        "def cleaning(sen):\n",
        "    cleanedSen = \"\"\n",
        "\n",
        "    charCount = len(sen)\n",
        "\n",
        "    # For full stop sentences\n",
        "    for i in range(charCount):\n",
        "      # For Question Mark Sentences\n",
        "      if(sen[i] == '?'):\n",
        "        continue\n",
        "\n",
        "      if(sen[i] == '.'):\n",
        "        # For starting . case\n",
        "        if(i == 0):\n",
        "          continue\n",
        "\n",
        "        # Mr.\n",
        "        if(i-2 >= 0):\n",
        "          if(sen[i-1] == 'r' and sen[i-2] == 'm'):\n",
        "            cleanedSen += sen[i]\n",
        "            continue\n",
        "        if(i-2 >= 0):\n",
        "          if(sen[i-1] == 'r' and sen[i-2] == 'M'):\n",
        "            cleanedSen += sen[i]\n",
        "            continue\n",
        "        # Mrs\n",
        "        if(i-3 >= 0):\n",
        "          if(sen[i-1] == 's' and sen[i-2] == 'r' and sen[i-3] == 'm'):\n",
        "            cleanedSen += sen[i]\n",
        "            continue\n",
        "        if(i-3 >= 0):\n",
        "          if(sen[i-1] == 's' and sen[i-2] == 'r' and sen[i-3] == 'M'):\n",
        "            cleanedSen += sen[i]\n",
        "            continue\n",
        "        # For Case like N.H.P.C i.e. name of coorporation\n",
        "        # U.S. , U.K. i.e. countries and places\n",
        "        if(i-1 >= 0):\n",
        "          if(sen[i-1].isupper()):\n",
        "            cleanedSen += sen[i]\n",
        "            continue\n",
        "        # i.e.\n",
        "        if(i-1 >= 0 and i+2 < charCount):\n",
        "          if(sen[i-1] == 'i' and sen[i+1] == 'e' and sen[i+2] == '.'):\n",
        "            cleanedSen += sen[i]\n",
        "            continue\n",
        "        # Dr.\n",
        "        if(i-2 >= 0):\n",
        "          if(sen[i-1] == 'r' and sen[i-2] == 'd'):\n",
        "            cleanedSen += sen[i]\n",
        "            continue\n",
        "        if(i-2 >= 0):\n",
        "          if(sen[i-1] == 'r' and sen[i-2] == 'D'):\n",
        "            cleanedSen += sen[i]\n",
        "            continue\n",
        "        # St.\n",
        "        if(i-2 >= 0):\n",
        "          if(sen[i-1] == 't' and sen[i-2] == 's'):\n",
        "            cleanedSen += sen[i]\n",
        "            continue\n",
        "        if(i-2 >= 0):\n",
        "          if(sen[i-1] == 't' and sen[i-2] == 'S'):\n",
        "            cleanedSen += sen[i]\n",
        "            continue\n",
        "        # Jr.\n",
        "        if(i-2 >= 0):\n",
        "          if(sen[i-1] == 'r' and sen[i-2] == 'j'):\n",
        "            cleanedSen += sen[i]\n",
        "            continue\n",
        "        if(i-2 >= 0):\n",
        "          if(sen[i-1] == 'r' and sen[i-2] == 'J'):\n",
        "            cleanedSen += sen[i]\n",
        "            continue\n",
        "        # Prof.\n",
        "        if(i-4 >= 0):\n",
        "          if(sen[i-1] == 'f' and sen[i-2] == 'o' and sen[i-3] == 'r' and sen[i-4] == 'p'):\n",
        "            cleanedSen += sen[i]\n",
        "            continue\n",
        "        if(i-4 >= 0):\n",
        "          if(sen[i-1] == 'f' and sen[i-2] == 'o' and sen[i-3] == 'r' and sen[i-4] == 'P'):\n",
        "            cleanedSen += sen[i]\n",
        "            continue\n",
        "        # Rev.\n",
        "        if(i-3 >= 0):\n",
        "          if(sen[i-1] == 'v' and sen[i-2] == 'e' and sen[i-3] == 'r'):\n",
        "            cleanedSen += sen[i]\n",
        "            continue\n",
        "        if(i-3 >= 0):\n",
        "          if(sen[i-1] == 'v' and sen[i-2] == 'e' and sen[i-3] == 'R'):\n",
        "            cleanedSen += sen[i]\n",
        "            continue\n",
        "      else:\n",
        "        cleanedSen += sen[i]\n",
        "\n",
        "    cleanedSen = cleanedSen.lower()\n",
        "    remChar = '[\\',/<\\\">?;:''\\\"\\\\|{}[]-=_+~!@#$%^&*()`]'\n",
        "    cleanedSen2 = \"\"\n",
        "    for i in  cleanedSen:\n",
        "      if(i in remChar):\n",
        "        cleanedSen2 += \" \"\n",
        "        continue\n",
        "      else:\n",
        "        cleanedSen2 += i\n",
        "    return cleanedSen2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0LV4JVlgt1Lm"
      },
      "outputs": [],
      "source": [
        "def similarity(sen1, sen2):\n",
        "    return 0.7 * semantic_similarity(sen1, sen2) + (0.3) * word_order_similarity(sen1, sen2)\n",
        "\n",
        "\n",
        "def semantic_similarity(sen1, sen2):\n",
        "    sen1 = cleaning(sen1)\n",
        "    sen2 = cleaning(sen2)\n",
        "    words_1 = nltk.word_tokenize(sen1)\n",
        "    words_2 = nltk.word_tokenize(sen2)\n",
        "    joint_words = joint_set(sen1,sen2)\n",
        "    vec_1 = semantic_vector(words_1, joint_words)\n",
        "    vec_2 = semantic_vector(words_2, joint_words)\n",
        "    return np.dot(vec_1, vec_2) / (np.linalg.norm(vec_1) * np.linalg.norm(vec_2))\n",
        "\n",
        "\n",
        "def word_order_similarity(sen1, sen2):\n",
        "    words_1 = nltk.word_tokenize(sen1)\n",
        "    words_2 = nltk.word_tokenize(sen2)\n",
        "    joint_words = joint_set(sen1,sen2)\n",
        "    r1 = wordOrder_vector(words_1, joint_words)\n",
        "    r2 = wordOrder_vector(words_2, joint_words)\n",
        "    diff_vec = [None] * len(joint_words)\n",
        "    sum_vec = [None] * len(joint_words)\n",
        "    for i in range(len(joint_words)):\n",
        "      diff_vec[i] = r1[i]-r2[i]\n",
        "    for i in range(len(joint_words)):\n",
        "      sum_vec[i] = r1[i]+r2[i]\n",
        "    if(np.linalg.norm(sum_vec) == 0):\n",
        "      return 0\n",
        "    ans = 1-(np.linalg.norm(diff_vec) / np.linalg.norm(sum_vec))\n",
        "    return ans\n",
        "\n",
        "def isSim (phrase1,phrase2):\n",
        "\n",
        "  score=similarity(phrase1,phrase2)\n",
        "\n",
        "\n",
        "  if score>=0.6:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZukHwXHHwXs1",
        "outputId": "76c1e3d8-762e-42c6-cff9-6457c6aad7e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['phrase1', 'phrase2', 'sentence1', 'sentence2', 'label', 'idx'],\n",
            "        num_rows: 7004\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['phrase1', 'phrase2', 'sentence1', 'sentence2', 'label', 'idx'],\n",
            "        num_rows: 1000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['phrase1', 'phrase2', 'sentence1', 'sentence2', 'label', 'idx'],\n",
            "        num_rows: 2000\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print (dataset2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8esxzUFMyFpv",
        "outputId": "42dc4003-637f-4fdd-e7fa-97a2bfb2921b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set accuracy: 0.509\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "dataset2 = load_dataset(\"PiC/phrase_similarity\")\n",
        "\n",
        "# Define the isSim function\n",
        "def isSim(phrase1, phrase2):\n",
        "    score = similarity(phrase1, phrase2)\n",
        "    if score >= 0.7:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "def calculate_accuracy(dataset):\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    for example in dataset:\n",
        "        phrase1 = example[\"phrase1\"]\n",
        "        phrase2 = example[\"phrase2\"]\n",
        "        label = example[\"label\"]\n",
        "        prediction = isSim(phrase1, phrase2)\n",
        "        if prediction == label:\n",
        "            correct_predictions += 1\n",
        "        total_predictions += 1\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    return accuracy\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "test_accuracy = calculate_accuracy(dataset2[\"test\"])\n",
        "print(\"Test set accuracy:\", test_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SR8hMSTS2Nk7"
      },
      "outputs": [],
      "source": [
        "test_accuracy = calculate_accuracy(dataset2[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSwRSaf02R4F",
        "outputId": "777c46fb-f028-46e4-dbd9-53e86d3ca69a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'sentence1', 'sentence2', 'label'],\n",
            "        num_rows: 49401\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'sentence1', 'sentence2', 'label'],\n",
            "        num_rows: 8000\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'sentence1', 'sentence2', 'label'],\n",
            "        num_rows: 8000\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMSszP5n2W_m",
        "outputId": "d75c87eb-5303-4817-b149-cc0c4ccaa45c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set accuracy: 0.442\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define the isSim function\n",
        "def isSim(phrase1, phrase2):\n",
        "    score = similarity(phrase1, phrase2)\n",
        "    if score >= 0.7:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Define a function to calculate accuracy\n",
        "def calculate_accuracy(dataset):\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    for example in dataset:\n",
        "        phrase1 = example[\"sentence1\"]\n",
        "        phrase2 = example[\"sentence2\"]\n",
        "        label = example[\"label\"]\n",
        "        prediction = isSim(phrase1, phrase2)\n",
        "        if prediction == label:\n",
        "            correct_predictions += 1\n",
        "        total_predictions += 1\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    return accuracy\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "test_accuracy = calculate_accuracy(dataset[\"test\"])\n",
        "print(\"Test set accuracy:\", test_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odXkwIKY4uSa",
        "outputId": "2c3e045b-6089-4c31-eef6-03cbd9fe5796"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Validation Loss: 0.25860340230994755\n",
            "Epoch 2/3, Validation Loss: 0.2520031813118193\n"
          ]
        }
      ],
      "source": [
        "from datasets import DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "\n",
        "data = dataset2\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming train_data is a dictionary\n",
        "train_df = pd.DataFrame(train_data)\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "train_data = train_df.to_dict(orient='records')\n",
        "\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.2, shuffle=True, random_state=42)\n",
        "\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.2, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "# Define a Dataset class\n",
        "class SimilarityDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        phrase1 = item['phrase1']\n",
        "        phrase2 = item['phrase2']\n",
        "        sentence1 = item['sentence1']\n",
        "        sentence2 = item['sentence2']\n",
        "        label = item['label']\n",
        "\n",
        "        encoding = self.tokenizer(phrase1, phrase2, sentence1, sentence2, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
        "        return {'input_ids': encoding['input_ids'].squeeze(0), 'attention_mask': encoding['attention_mask'].squeeze(0), 'label': torch.tensor(label, dtype=torch.float)}\n",
        "\n",
        "# Load the BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
        "\n",
        "# Define the dataset and data loaders\n",
        "max_length = 128\n",
        "train_dataset = SimilarityDataset(train_data, tokenizer, max_length)\n",
        "val_dataset = SimilarityDataset(val_data, tokenizer, max_length)\n",
        "test_dataset = SimilarityDataset(test_data, tokenizer, max_length)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Fine-tune the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    for batch in val_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_losses.append(outputs.loss.item())\n",
        "\n",
        "    val_loss = sum(val_losses) / len(val_losses)\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Validation Loss: {val_loss}')\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "predictions = []\n",
        "labels = []\n",
        "for batch in test_loader:\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    label_batch = batch['label'].to('cpu').numpy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits.squeeze(1)\n",
        "        predictions.extend(logits.cpu().numpy())\n",
        "        labels.extend(label_batch)\n",
        "\n",
        "rmse = sqrt(mean_squared_error(labels, predictions))\n",
        "print(f'RMSE on Test Set: {rmse}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "vp4DbB-o5USY",
        "outputId": "e6fac734-f150-4ed7-9e19-1bde24f59621"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.24.4\n",
            "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 15.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.4\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install numpy==1.24.4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPyJdTLV7FYf",
        "outputId": "23c1ac00-c769-4895-fe3c-a99906eeaa55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['phrase1', 'phrase2', 'sentence1', 'sentence2', 'label', 'idx'])\n"
          ]
        }
      ],
      "source": [
        "print(train_data.keys())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}